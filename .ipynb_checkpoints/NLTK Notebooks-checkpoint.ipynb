{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tokenizing words and sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello Mr. Smith, how are you doing today?', 'The Weather is great.']\n"
     ]
    }
   ],
   "source": [
    "example_text = \"Hello Mr. Smith, how are you doing today? The Weather is great.\"\n",
    "\n",
    "# split the sentences. \n",
    "print(sent_tokenize(example_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Mr.', 'Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'Weather', 'is', 'great', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(example_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example_sentence = \"This is an example showing off stop words filtration.\"\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'am', 'during', 'is', 'y', 'out', 'weren', 'doesn', 'up', 'for', 'when', 'ain', 'because', 'be', 'needn', 'a', 'here', 'we', 'i', 'between', 'same', 'can', 'ours', 'which', 'his', 'from', 'mustn', 'do', 'over', 'some', 'few', 'yourselves', 'once', 'hers', 'he', 's', 'couldn', 'down', 'where', 'as', 'hadn', 'has', 'below', 'this', 'having', 're', 'above', 'did', 'it', 'o', 'these', 'their', 'and', 'own', 'but', 'in', 'into', 'does', 't', 'hasn', 'shouldn', 'off', 'why', 'aren', 'shan', 'other', 'him', 've', 'yourself', 'was', 'she', 'while', 'only', 'ma', 'or', 'too', 'her', 'mightn', 'if', 'further', 'not', 'being', 'will', 'should', 'on', 'had', 'whom', 'then', 'who', 'all', 'isn', 'what', 'theirs', 'itself', 'your', 'both', 'my', 'wouldn', 'at', 'that', 'its', 'now', 'no', 'won', 'ourselves', 'been', 'about', 'haven', 'so', 'the', 'them', 'to', 'just', 'more', 'wasn', 'very', 'are', 'how', 'were', 'an', 'doing', 'don', 'herself', 'of', 'those', 'me', 'didn', 'd', 'they', 'll', 'such', 'yours', 'm', 'under', 'after', 'through', 'any', 'by', 'against', 'with', 'you', 'themselves', 'before', 'nor', 'than', 'until', 'each', 'most', 'myself', 'himself', 'there', 'again', 'our', 'have'}\n"
     ]
    }
   ],
   "source": [
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'example', 'showing', 'stop', 'words', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(example_sentence)\n",
    "filtered_sentence = []\n",
    "for w in words:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'example', 'showing', 'stop', 'words', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "filtered_sentence = [w for w in words if w not in stop_words]\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "python\n",
      "python\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "\n",
    "example_words = ['python','pythonic','pythoning']\n",
    "\n",
    "for w in example_words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.  Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_text =  state_union.raw('2005-GWBush.txt')\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            print(tagged)\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "#process_content()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "POS tag list:\n",
    "\n",
    "CC\tcoordinating conjunction\n",
    "CD\tcardinal digit\n",
    "DT\tdeterminer\n",
    "EX\texistential there (like: \"there is\" ... think of it like \"there exists\")\n",
    "FW\tforeign word\n",
    "IN\tpreposition/subordinating conjunction\n",
    "JJ\tadjective\t'big'\n",
    "JJR\tadjective, comparative\t'bigger'\n",
    "JJS\tadjective, superlative\t'biggest'\n",
    "LS\tlist marker\t1)\n",
    "MD\tmodal\tcould, will\n",
    "NN\tnoun, singular 'desk'\n",
    "NNS\tnoun plural\t'desks'\n",
    "NNP\tproper noun, singular\t'Harrison'\n",
    "NNPS\tproper noun, plural\t'Americans'\n",
    "PDT\tpredeterminer\t'all the kids'\n",
    "POS\tpossessive ending\tparent's\n",
    "PRP\tpersonal pronoun\tI, he, she\n",
    "PRP$\tpossessive pronoun\tmy, his, hers\n",
    "RB\tadverb\tvery, silently,\n",
    "RBR\tadverb, comparative\tbetter\n",
    "RBS\tadverb, superlative\tbest\n",
    "RP\tparticle\tgive up\n",
    "TO\tto\tgo 'to' the store.\n",
    "UH\tinterjection\terrrrrrrrm\n",
    "VB\tverb, base form\ttake\n",
    "VBD\tverb, past tense\ttook\n",
    "VBG\tverb, gerund/present participle\ttaking\n",
    "VBN\tverb, past participle\ttaken\n",
    "VBP\tverb, sing. present, non-3d\ttake\n",
    "VBZ\tverb, 3rd person sing. present\ttakes\n",
    "WDT\twh-determiner\twhich\n",
    "WP\twh-pronoun\twho, what\n",
    "WP$\tpossessive wh-pronoun\twhose\n",
    "WRB\twh-abverb\twhere, when\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.  Chunking with Regular Expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/regxcheatsheet.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP Mr./NNP Speaker/NNP)\n",
      "  ,/,\n",
      "  (NP Vice/NNP President/NNP Cheney/NNP)\n",
      "  ,/,\n",
      "  members/NNS\n",
      "  (NP Congress/NNP)\n",
      "  ,/,\n",
      "  members/NNS\n",
      "  (NP Supreme/NNP Court/NNP)\n",
      "  (NP diplomatic/JJ corps/NN)\n",
      "  ,/,\n",
      "  distinguished/JJ\n",
      "  guests/NNS\n",
      "  ,/,\n",
      "  fellow/JJ\n",
      "  citizens/NNS\n",
      "  :/:\n",
      "  (NP Today/NN)\n",
      "  (NP nation/NN)\n",
      "  lost/VBD\n",
      "  beloved/VBN\n",
      "  ,/,\n",
      "  graceful/JJ\n",
      "  ,/,\n",
      "  (NP courageous/JJ woman/NN)\n",
      "  called/VBN\n",
      "  (NP America/NNP)\n",
      "  founding/VBG\n",
      "  ideals/NNS\n",
      "  carried/VBD\n",
      "  (NP noble/JJ dream/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "grammer=r\"\"\"NP: {<DT|pp\\$>?<JJ>*<NN>}\n",
    "                {<NNP>+}\"\"\"\n",
    "cp = nltk.RegexpParser(grammer)\n",
    "for i in tokenized[1:2]:\n",
    "    words = nltk.word_tokenize(i)\n",
    "    # discard stop words\n",
    "    words_nonstop = [w for w in words if w not in stop_words]\n",
    "    tagged = nltk.pos_tag(words_nonstop)\n",
    "    print(cp.parse(tagged))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Chinking\n",
    "<p>Chinking is a part of the chuncking process. A chink is what we wish to remove from chunk</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP\n",
      "    Mr./NNP\n",
      "    Speaker/NNP\n",
      "    ,/,\n",
      "    Vice/NNP\n",
      "    President/NNP\n",
      "    Cheney/NNP\n",
      "    ,/,\n",
      "    members/NNS\n",
      "    Congress/NNP\n",
      "    ,/,\n",
      "    members/NNS\n",
      "    Supreme/NNP\n",
      "    Court/NNP\n",
      "    diplomatic/JJ\n",
      "    corps/NN\n",
      "    ,/,\n",
      "    distinguished/JJ\n",
      "    guests/NNS\n",
      "    ,/,\n",
      "    fellow/JJ\n",
      "    citizens/NNS\n",
      "    :/:\n",
      "    Today/NN\n",
      "    nation/NN)\n",
      "  lost/VBD\n",
      "  beloved/VBN\n",
      "  (NP ,/, graceful/JJ ,/, courageous/JJ woman/NN)\n",
      "  called/VBN\n",
      "  (NP America/NNP)\n",
      "  founding/VBG\n",
      "  (NP ideals/NNS)\n",
      "  carried/VBD\n",
      "  (NP noble/JJ dream/NN ./.))\n"
     ]
    }
   ],
   "source": [
    "grammer=r\"\"\"NP: {<.*>+}\n",
    "                }<VB.?|IN|DT>+{\"\"\"\n",
    "cp = nltk.RegexpParser(grammer)\n",
    "for i in tokenized[1:2]:\n",
    "    words = nltk.word_tokenize(i)\n",
    "    # discard stop words\n",
    "    words_nonstop = [w for w in words if w not in stop_words]\n",
    "    tagged = nltk.pos_tag(words_nonstop)\n",
    "    chuncked = cp.parse(tagged)\n",
    "    print(chuncked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Name Entity Recognition\n",
    "<p>Name entity recognition is useful to quickly find out what the subjects of discussions are.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Mr./NNP Speaker/NNP)\n",
      "  ,/,\n",
      "  Vice/NNP\n",
      "  President/NNP\n",
      "  (PERSON Cheney/NNP)\n",
      "  ,/,\n",
      "  members/NNS\n",
      "  (ORGANIZATION Congress/NNP)\n",
      "  ,/,\n",
      "  members/NNS\n",
      "  (ORGANIZATION Supreme/NNP Court/NNP)\n",
      "  diplomatic/JJ\n",
      "  corps/NN\n",
      "  ,/,\n",
      "  distinguished/JJ\n",
      "  guests/NNS\n",
      "  ,/,\n",
      "  fellow/JJ\n",
      "  citizens/NNS\n",
      "  :/:\n",
      "  Today/NN\n",
      "  nation/NN\n",
      "  lost/VBD\n",
      "  beloved/VBN\n",
      "  ,/,\n",
      "  graceful/JJ\n",
      "  ,/,\n",
      "  courageous/JJ\n",
      "  woman/NN\n",
      "  called/VBN\n",
      "  (GPE America/NNP)\n",
      "  founding/VBG\n",
      "  ideals/NNS\n",
      "  carried/VBD\n",
      "  noble/JJ\n",
      "  dream/NN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "cp = nltk.RegexpParser(grammer)\n",
    "for i in tokenized[1:2]:\n",
    "    words = nltk.word_tokenize(i)\n",
    "    # discard stop words\n",
    "    words_nonstop = [w for w in words if w not in stop_words]\n",
    "    tagged = nltk.pos_tag(words_nonstop)\n",
    "    nameEnt = nltk.ne_chunk(tagged)\n",
    "    print(nameEnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Lemmatizing\n",
    "LEMMATIZING is a very similar operation to stemming. The major difference is, as you saw earlier, stemming can often create non-existent words. \n",
    "**Group words together**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize(\"better\",pos=\"a\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "sample_text = gutenberg.raw(\"bible-kjv.txt\")\n",
    "tokenize = sent_tokenize(sample_text)\n",
    "#print(tokenize[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. WordNet\n",
    "With WordNet we can do things like look up words and their meaning according to their parts of speech, we can find synonyms, antonyms, and even examples of the word in use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('plan.n.01'), Synset('program.n.02'), Synset('broadcast.n.02'), Synset('platform.n.02'), Synset('program.n.05'), Synset('course_of_study.n.01'), Synset('program.n.07'), Synset('program.n.08'), Synset('program.v.01'), Synset('program.v.02')]\n"
     ]
    }
   ],
   "source": [
    "syns = wordnet.synsets(\"program\")\n",
    "print(syns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plan\n"
     ]
    }
   ],
   "source": [
    "print(syns[0].lemmas()[0].name()) # just the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a series of steps to be carried out or goals to be accomplished\n"
     ]
    }
   ],
   "source": [
    "print(syns[0].definition()) #definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['they drew up a six-step plan', 'they discussed plans for a new bond issue']\n"
     ]
    }
   ],
   "source": [
    "print(syns[0].examples()) #example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'in_force', 'honorable', 'goodness', 'safe', 'undecomposed', 'thoroughly', 'full', 'upright', 'serious', 'secure', 'dear', 'salutary', 'skillful', 'proficient', 'commodity', 'good', 'unspoiled', 'near', 'just', 'practiced', 'sound', 'beneficial', 'honest', 'skilful', 'dependable', 'unspoilt', 'in_effect', 'adept', 'expert', 'well', 'soundly', 'respectable', 'estimable', 'trade_good', 'right', 'ripe', 'effective'}\n",
      "{'evilness', 'evil', 'bad', 'badness', 'ill'}\n"
     ]
    }
   ],
   "source": [
    "# synonyms and antonyms \n",
    "synonyms = []\n",
    "antonyms = []\n",
    "for syn in wordnet.synsets(\"good\"):\n",
    "    for l in syn.lemmas():\n",
    "        #print(l.name())\n",
    "        synonyms.append(l.name())\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    "print(set(synonyms))\n",
    "print(set(antonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9090909090909091\n"
     ]
    }
   ],
   "source": [
    "# similarity comparison between two words\n",
    "w1 = wordnet.synset(\"ship.n.01\")\n",
    "w2 = wordnet.synset(\"boat.n.01\")\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Text Classification \n",
    "we're gonna classify the movie reviews into pos and neg categories based on the words distributions. \n",
    "\n",
    "It's a naive algorithm for now!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['we', 'could', 'paraphrase', 'michelle', 'pfieffer', ...], 'neg')\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "documents = [(movie_reviews.words(fileid),category)\n",
    "              for category in movie_reviews.categories()\n",
    "              for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "\n",
    "random.shuffle(documents)\n",
    "print(documents[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 77717), ('the', 76529), ('.', 65876), ('a', 38106), ('and', 35576), ('of', 34123), ('to', 31937), (\"'\", 30585), ('is', 25195), ('in', 21822), ('s', 18513), ('\"', 17612), ('it', 16107), ('that', 15924), ('-', 15595)]\n"
     ]
    }
   ],
   "source": [
    "all_words = [w.lower() for w in movie_reviews.words()]\n",
    "all_words = nltk.FreqDist(all_words)\n",
    "print(all_words.most_common(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above list of words are useless, we'll improve it in later sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253\n"
     ]
    }
   ],
   "source": [
    "print (all_words[\"stupid\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Word as Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_features = list(all_words.keys())[:3000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the every words in the text is in the top 3000 words in the review corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_features(document):\n",
    "    words = set(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w]=(w in words)\n",
    "    return features\n",
    "#print(find_features(movie_reviews.words('neg/cv000_29416.txt')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(find_features(rev),category) for (rev,category) in documents]\n",
    "#print(featuresets[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Naive Bayes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
